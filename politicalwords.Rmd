---
title: "Analyse sémantique des partis politiques francais"
author: "Paul Charlet"
date: "2019-11-02"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE, cache = TRUE)
library(ggraph)
library(igraph)
```

## Introduction

Nous allons analyser le contenu texte de 3 partis politiques francais pour voir comment ils diffèrent. Nous pourrons observer : les réseaux lexicaux, les sentiments, les sujets abordés. À la fin, cela nous permettra d'envisager le machine learning pour reconnaitre la couleur politique des textes.

Le code source est disponible sur https://github.com/paul-chrlt/political-words

## Acquisition des données : scrapping

La première étape consistera à automatiser la lecture des sites internet. Pour chacun :

* aller sur la page listant les actualités et récupérer un tableau contenant un titre, un lien.
* aller sur la page suivante et recommencer, jusqu'à avoir une quantité de liens suffisante.
* aller sur chaque lien récupéré pour récupérer les textes des articles en différenciant le corps de texte, le titre, la date de publication.

Nous allons créer 3 fonctions différentes pour analyser 3 partis. Ces fonctions ne sont pas appelées directement dans ce document pour éviter de solliciter abusivement les serveurs de ces 3 sites. Vous retrouverez le fonctionnement dans les sources : `scraper_rn.R`, `scraper_lrm.R`, `scraper_eelv.R`.

```{r}
load('articlesRN.Rdata')
load('articlesEELV.Rdata')
load('articlesLRM.Rdata')
```

## Nettoyage

Commencons par rassembler et formater l'ensemble de ces données.  
Pendant le scrapping, certaines données ont pu être en doublon, nous allons aussi nettoyer ca.
```{r}
articlesRN$parti <- "RN"
articlesEELV$parti <- "EELV"
articlesLRM$parti <- "LRM"
articles <- rbind(articlesRN,articlesEELV,articlesLRM,stringsAsFactors=FALSE)
articles$articleDate <- as.Date(articles$articleDate, format = "%d %B %Y")
articles$parti <- factor(articles$parti)
articles <- unique(articles,MARGIN=1)
```

## Périodicité

Nous allons déterminer les périodes de pression selon les partis. Pour ca, nous allons créer 2 colonnes supplémentaires, correspondant aux mois et aux jours de la semaine.
```{r}
library(lubridate)
articles$month <- month(articles$articleDate,label = TRUE)

library(ggplot2)
monthSeasonalityPlot <- ggplot(data = articles,aes(x=month,fill=parti)) + geom_bar(position=position_dodge2(preserve = "single")) + facet_grid(parti~.) + theme_light()
monthSeasonalityPlot
```
```{r}
articles$day <- day(articles$articleDate)
daySeasonalityPlot <- ggplot(data = articles,aes(x=day,fill=parti)) + geom_bar(position=position_dodge2(preserve = "single")) + facet_grid(parti~.) + theme_light()
daySeasonalityPlot
```

```{r}
articles$weekDay <- wday(articles$articleDate,label = TRUE)
weekDaySeasonalityPlot <- ggplot(data = articles,aes(x=weekDay,fill=parti)) + geom_bar(position=position_dodge2(preserve = "single")) + facet_grid(parti~.) + theme_light()
weekDaySeasonalityPlot
```

## Préparation des données pour analyse NLP : UDpipe

Nous allons ensuite utiliser UDpipe pour démarrer une analyse du contenu des articles. Nous allons l'utiliser pour identifier dans nos textes :

* le radical de chaque mot : pour que les accords et la conjugaison ne perturbent pas l'analyse
* la fonction grammaticale de chaque mot : nous pourrons par exemple choisir d'exclure les prépositions

Il faudra lui fournir un modèle correspondant à la langue francaise. Nous le téléchargeons une fois sur le disque avec la fonction `udpipe_download_model`.

```{r}
library(udpipe)
library(dplyr)
if(!file.exists("french-gsd-ud-2.4-190531.udpipe")){udpipe_download_model("french")}
udpipeModel <- udpipe_load_model("french-gsd-ud-2.4-190531.udpipe")
```

Il nous reste alors à préparer les données pour nos 3 ensembles de textes :

```{r}
udpipedEELV <- udpipe(articles$articleContent[articles$parti=="EELV"],udpipeModel)
udpipedRN <- udpipe(articles$articleContent[articles$parti=="RN"],udpipeModel)
udpipedLRM <- udpipe(articles$articleContent[articles$parti=="LRM"],udpipeModel)
```

## Réseaux lexicaux

Nous pouvons commencer par regarder les mots souvent présents ensembles. Cela nous permettra d'identifier de grands thèmes ou expressions. Créons la fonction `viewCoocurences()` qui prendra en entrée des objets UDpipe et retournera un graphique des coocurences pour ces objets.

```{r,fig.height=24,fig.width=8}
library(cowplot)
coocurences <- list()
viewCoocurences <- function(udpipeObjects,coocLimitation=40){
    for(i in 1:length(udpipeObjects)){
        coocurences[[i]] <- cooccurrence(x = subset(udpipeObjects[[i]], upos %in% c("NOUN", "ADJ")), 
                                       term = "lemma",
                                       group = c("sentence_id"),
                                       skipgram = 4)
    }
    wordnetwork <- lapply(coocurences, head,coocLimitation)
    wordnetwork <- lapply(wordnetwork, graph_from_data_frame)
    coocGraphs <- list()
    for (i in 1:length(wordnetwork)) {
        coocGraphs[[i]] <- ggraph(wordnetwork[[i]], layout = "fr") +
            geom_edge_link(aes(width = cooc, edge_alpha = cooc), edge_colour = "lightgreen") +
            geom_node_text(aes(label = name), col = "black", size = 4) +
            theme(legend.position = "none") +
            labs(title = paste("Cooccurrences des mots,",names(udpipeObjects[i])), subtitle = "Nouns & Adjective")
    }
    plot_grid(plotlist = coocGraphs, labels = "",ncol=1) + theme_void()
}
udpipeObjects <- list("EELV"=udpipedEELV,"LRM"=udpipedLRM,"RN"=udpipedRN)
viewCoocurences(udpipeObjects,50)
```


## Sentiments négatifs

```{r}
library(magrittr)
load(file("https://github.com/sborms/sentometrics/raw/master/data-raw/FEEL_fr.rda"))
load(file("https://github.com/sborms/sentometrics/raw/master/data-raw/valence-raw/valShifters.rda"))

sentimentsGraph <- function(udpipedText, graphTitle){
  polarity_terms <- rename(FEEL_fr, term = x, polarity = y)
  polarity_negators <- subset(valShifters$valence_fr, t == 1)$x
  polarity_amplifiers <- subset(valShifters$valence_fr, t == 2)$x
  polarity_deamplifiers <- subset(valShifters$valence_fr, t == 3)$x
  
  sentiments <- txt_sentiment(udpipedText, term = "lemma", 
                              polarity_terms = polarity_terms,
                              polarity_negators = polarity_negators, 
                              polarity_amplifiers = polarity_amplifiers,
                              polarity_deamplifiers = polarity_deamplifiers)
  sentiments <- sentiments$data
  reasons <- sentiments %>% 
    cbind_dependencies() %>%
    select(doc_id, lemma, token, upos, sentiment_polarity, token_parent, lemma_parent, upos_parent, dep_rel) %>% filter(sentiment_polarity < 0)
  reasons <- filter(reasons, dep_rel %in% "amod")
  word_cooccurences <- reasons %>% 
    group_by(lemma, lemma_parent) %>%
    summarise(cooc = n()) %>%
    arrange(-cooc)
  vertices <- bind_rows(
    data_frame(key = unique(reasons$lemma)) %>% mutate(in_dictionary = if_else(key %in% polarity_terms$term, "in_dictionary", "linked-to")),
    data_frame(key = unique(setdiff(reasons$lemma_parent, reasons$lemma))) %>% mutate(in_dictionary = "linked-to"))
  cooc <- head(word_cooccurences, 50)
  set.seed(123456789)
  graphicLexicalNetworks <- cooc %>%  
    graph_from_data_frame(vertices = filter(vertices, key %in% c(cooc$lemma, cooc$lemma_parent))) %>%
    ggraph(layout = "fr") +
    geom_edge_link0(aes(edge_alpha = cooc, edge_width = cooc)) +
    geom_node_point(aes(colour = in_dictionary), size = 5) +
    geom_node_text(aes(label = name), vjust = 1.8, col = "darkgreen") +
    ggtitle(graphTitle) +
    theme_void()
  graphicLexicalNetworks
}
sentimentsGraph(udpipedEELV,"réseaux lexicaux à EELV dans les phrases analysées comme négatives")
sentimentsGraph(udpipedRN,"réseaux lexicaux à RN dans les phrases analysées comme négatives")
sentimentsGraph(udpipedLRM,"réseaux lexicaux à LRM dans les phrases analysées comme négatives")
```

